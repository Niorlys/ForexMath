{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Variables \n",
    " \n",
    "When performing experiments, often we are interested in a quantitative characterization of the outcomes instead of the outcomes themselves. For example, when we toss two dice, we might be interested in the sum of the numbers that come up. In this case, we can build a function $X$ where $X(\\omega) = X((i,j)) = i+j$. Then, for instance, the event $A = \\{(1,2), (2,1)\\}$ can be stated as $\\{ \\omega: X(\\omega) = 3\\} = X^{-1}(\\{3\\})$. For assigning probabilities to the events mapped by $X$, we must allow $X$ to satisfy the \"natural\" condition specified by the following definition.  \n",
    "\n",
    "**Definition 2.1 (Measurable function)** Let $(\\Omega, \\mathcal{F})$ be a measurable space. A function $f: \\Omega \\to \\mathbb{R^p}$ is  $\\langle \\mathcal{F}, \\mathcal{B}(\\mathbb{R^p}) \\rangle$ measurable, if for every Borel set $B \\subset \\mathbb{R^p}$, we have\n",
    "$$f^{-1}(B) = \\{ \\omega \\in \\Omega: f(\\omega) \\in B \\} \\in \\mathcal{F}.$$  \n",
    " \n",
    "Often, for simplicity of notation, we will write $\\{ \\omega \\in \\Omega: f(\\omega) \\in B \\}$ as $\\{f \\in B\\}$.\n",
    " \n",
    "**Definition 2.2 (Random Variable)**  Let $(\\Omega, \\mathcal{F})$ be a measurable space. A function $X: \\Omega \\to \\mathbb{R^p}$ is called a random variable if it is $\\langle \\mathcal{F}, \\mathcal{B}(\\mathbb{R^p}) \\rangle$ measurable.\n",
    "  \n",
    "For simplicity, we take $p=1$ in the definition above, and when there is no ambiguity, we say only that $X$ is a random variable, assuming that preimages are taken with respect to the Borel $\\sigma$-algebra in $\\mathbb{R^p}$. We call the condition \"natural\" because, as we saw earlier, events defined through a random variable are described by the preimage of Borel sets. Since we assign a probability measure to these events, it is natural to require that the preimages of Borel sets be measurable. An inmediate example of randm variable is the indicator function of an event $A \\subset \\Omega$, given by $I_A(\\omega) = 1$ if $\\omega \\in A$ and $I_A(\\omega) = 0$ otherwise. If $B$ is any Borel set, then\n",
    "\n",
    "$$\n",
    "I_A^{-1}(B) = \n",
    "\\begin{cases}\n",
    "A, & \\text{if } 1 \\in B \\text{ and } 0 \\notin B,\\\\\n",
    "A^c, & \\text{if } 1 \\notin B \\text{ and } 0 \\in B,\\\\\n",
    "\\varnothing, & \\text{if } 1 \\notin B \\text{ and } 0 \\notin B,\\\\\n",
    "\\Omega, & \\text{if } 1 \\in B \\text{ and } 0 \\in B.\n",
    "\\end{cases}\n",
    "$$ \n",
    " \n",
    "Since all the preimages of $I_A$ are in $\\mathcal{F}$, then $I_A$ is a random variable. However, verifying that every possible preimage of $X$ belongs to $\\mathcal{F}$  as we did for $I_A$ is impractical. Fortunately, we can check whether $X$ is a random variable by ensuring that this condition holds for a class $\\mathcal{C}$ of sets that generate the Borel $\\sigma$-algebra on $\\mathbb{R}$.\n",
    "\n",
    " \n",
    "**Theorem 2.1** \n",
    " \n",
    "Let $X: \\Omega \\to \\mathbb{R}$ be a function, and let $\\mathcal{C}$ a class of subsets of $\\mathbb{R}$ that generates the Borel $\\sigma$-algebra. Then, $X$ is a random variable if and only if for every $C \\in \\mathcal{C}$, we have\n",
    " $$X^{-1}(C) = \\{ \\omega \\in \\Omega: X(\\omega) \\in C \\} \\in \\mathcal{F}.$$  \n",
    "\n",
    "where $\\mathcal{F}$ is the $\\sigma$-algebra of the sample space $\\Omega$.\n",
    "  \n",
    "**Proof:**  $\\Rightarrow$ Let's define $\\mathcal{G}=\\{A \\in \\mathcal{B}(\\mathbb{R}): X^{-1}(A) \\in \\mathcal{F}\\}$. It is easy to prove that $\\mathcal{G}$ is a $\\sigma$-algebra. Since by assumption, $\\mathcal{C} \\subset \\mathcal{G}$, we have that $\\mathcal{B}(\\mathbb{R}) = \\sigma(\\mathcal{C}) \\subset \\mathcal{G}\\subset \\mathcal{B}(\\mathbb{R})$. Therefore, $\\mathcal{G}=\\mathcal{B}(\\mathbb{R})$, and $X$ is a random variable. The other direction is trivial. \n",
    "\n",
    "$\\blacksquare$ \n",
    " \n",
    "**Example 2.1** Let $\\{X_i\\}_{i=1}^n$ a finite collection of random variables respect to $\\mathcal{F}$, with $X_i:\\Omega\\rightarrow\\mathbb{R}^n$, then $X=(X_1,X_2,...,X_n)$ is a random variable respect in $\\langle \\mathcal{F}, \\mathcal{B}(\\mathbb{R}^n) \\rangle$. \n",
    " \n",
    "Since $\\mathcal{B}(\\mathbb{R}^n)$ is generated by the class $\\mathcal{C}$ of open rectangles  in $\\mathbb{R}^n$, then let $C = \\prod_{i=1}^n (a_i, b_i)$ be an open rectangle. Then, $\\forall C \\in \\mathcal{C}$, we have\n",
    "$$X^{-1}(C) = \\prod_{i=1}^n X_i^{-1}((a_i, b_i)) = \\bigcap_{i=1}^n \\{ \\omega: X_i(\\omega) \\in (a_i, b_i)\\} \\in \\mathcal{F}$$ \n",
    " \n",
    "since $\\{ \\omega: X_i(\\omega) \\in (a_i, b_i)\\}$ is in $\\mathcal{F}$ for all $i=1,2,...,n$ due to the fact that $X_i$ is a random variable. Therefore, $X$ is a random variable by Theorem 1.1.   $\\blacksquare$\n",
    "\n",
    "An useful case is when $\\mathcal{C}$ is the class of intervals of the form $(-\\infty, x]$. Theorerem 1.1 from the first article states that Borel's $\\sigma$-algebra is generated by the class $\\mathcal{C} = \\{(-\\infty, x]: x \\in \\mathbb{R}\\}$. This leads to the following corollary. \n",
    "\n",
    "**Corollary 2.2**\n",
    "\n",
    "Let $X: \\Omega \\to \\mathbb{R}$ be a function. Then, $X$ is a random variable if and only if for every $x \\in \\mathbb{R}$, we have\n",
    "$$X^{-1}((-\\infty, x]) = \\{ \\omega \\in \\Omega: X(\\omega) \\leq x \\} \\in \\mathcal{F}.$$  \n",
    "\n",
    "The above corollary is particularly useful to confirm that a function is a random variable, by stating that is sufficient to check if the preimage of intervals of the form $(-\\infty, x]$ is in $\\mathcal{F}$ as we show in the following example. \n",
    " \n",
    " \n",
    " **Example 2.2** Let $\\{X_n\\}_{n\\in\\mathbb{N}}$ a sequence of random variables in $\\langle \\mathcal{F}, \\mathcal{B}(\\mathbb{\\bar{R}}) \\rangle$. Then, $X = \\sup_{n\\in\\mathbb{N}} X_n$ is a random variable in $\\langle \\mathcal{F}, \\mathcal{B}(\\mathbb{\\bar{R}}) \\rangle$.   \n",
    "  \n",
    "For every $x \\in \\mathbb{R}$, we have\n",
    "  \n",
    "$$X^{-1}((-\\infty, x]) = \\{ \\omega: X(\\omega) \\leq x \\} = \\bigcap_{n\\in\\mathbb{N}}\\{\\omega: X_n(\\omega)\\leq x\\}=\\bigcap_{n\\in\\mathbb{N}} X_n^{-1}((-\\infty, x]) \\in \\mathcal{F}$$ \n",
    " \n",
    "since $X_n$ are random variables. Therefore, $X$ is a random variable by Corollary 1.2. The same argument can be used for the infimum, taking $\\inf_{n\\in\\mathbb{N}} X_n = - \\sup_{n\\in\\mathbb{N}} (-X_n)$. $\\blacksquare$ \n",
    " \n",
    "\n",
    " Now consider the following experiment: we have an urn with $n$ balls numbered from $1$ to $n$. We draw $k$ balls from the urn (without replacement), and let $X$ be the sum of the numbers on the drawn balls, if we set $A_k$ as the event where the ball with number $k$ is drawn, then $X = \\sum_{k=1}^n k I_{A_k}$. This is a basic example of a random variable expressed as function of another random variables. Indeed, we can generalize saying that functions of random variables are random variables.  \n",
    "  \n",
    "**Proposition 2.3** Let $X:\\Omega\\rightarrow\\mathbb{R^p}$ be a random variable and $g:\\mathbb{R^p}\\rightarrow\\mathbb{R^q}$ measurable in $\\langle \\mathcal{B}(\\mathbb{R^p}), \\mathcal{B}(\\mathbb{R^q}) \\rangle$. Then, $Y=g(X)$ is a random variable in $\\langle \\mathcal{F}, \\mathcal{B}(\\mathbb{R^q}) \\rangle$.  \n",
    " \n",
    "**Proof:** Left as an exercise. $\\blacksquare$  \n",
    " \n",
    "In order to apply the above proposition safely, we need to know when a real function is measurable. A sufficient condition is given by the following proposition.\n",
    " \n",
    "**Proposition 2.4** Let $g:\\mathbb{R^p}\\rightarrow\\mathbb{R^q}$ be continuous. Then, $g$ is measurable in $\\langle \\mathcal{B}(\\mathbb{R^p}), \\mathcal{B}(\\mathbb{R^q}) \\rangle$.\n",
    " \n",
    "**Proof:** Left as an exercise. Hint: use the fact that the preimage of an open set under a continuous function is open. $\\blacksquare$ \n",
    " \n",
    " **Example 2.3** If we have $n$ random variables $\\{X_k\\}_{k=1}^n$ in $\\langle \\mathcal{F}, \\mathcal{B}(\\mathbb{R}) \\rangle$, then $Y = X_1 + X_2 + ... + X_n$ is a random variable in $\\langle \\mathcal{F}, \\mathcal{B}(\\mathbb{R}) \\rangle$. \n",
    "  \n",
    "To see this, recall by **Exampple 1.3** that $X=(X_1, X_2, ..., X_n)$ is a random variable in $\\langle \\mathcal{F}, \\mathcal{B}(\\mathbb{R}^n) \\rangle$. Also, the function $g(x_1, x_2, ..., x_n) = x_1 + x_2 + ... + x_n$ is continuous in $\\mathbb{R}^n$ and therefore measurable, then by **Proposition 1.5**, $Y=g(X) = X_1 + X_2 + ... + X_n$ is a random variable in $\\langle \\mathcal{F}, \\mathcal{B}(\\mathbb{R}) \\rangle$. $\\blacksquare$  \n",
    " \n",
    "Another important examples of random variables are presented in the following proposition. The reader should be able to prove the following proposition by himself. \n",
    "\n",
    "**Proposition 2.5** Let $X_n:\\Omega\\rightarrow\\mathbb{R}$ be a random variable $\\forall n\\in\\mathbb{N}$, then the following functions ae random variables: \n",
    "\n",
    "i. $\\sup_{n\\in\\mathbb{N}} X_n$ \n",
    "\n",
    "ii. $\\inf_{n\\in\\mathbb{N}} X_n$ \n",
    "\n",
    "iii. $\\limsup_{n\\in\\mathbb{N}} X_n$ \n",
    "\n",
    "iv. $\\liminf_{n\\in\\mathbb{N}} X_n$ \n",
    "\n",
    "\n",
    "**Proof:** Left as an exercise. $\\blacksquare$\n",
    "\n",
    "\n",
    " ## Distribution of a random variable\n",
    " Recall that we use random variables because most of the time we are interested in a quantitative characterization of the outcomes instead of the outcomes themselves. A random variable allows us to abstract ourselves from the original sample space and simply work with the Borel sets. We know that $X^{-1}(B)$ is measurable for every $B \\in \\mathcal{B}(\\mathbb{R})$, so we can define a new probability measure on $\\mathcal{B}(\\mathbb{R})$ as follows: \n",
    "  \n",
    "$$\n",
    "P_X(B) = P(X^{-1}(B)) = P(\\{ \\omega \\in \\Omega: X(\\omega) \\in B \\}) = P(X\\in B)\n",
    "$$\n",
    "\n",
    " It is easy to check that $P_X$ is a probability measure on $\\mathcal{B}(\\mathbb{R})$.  \n",
    "\n",
    " **Definition 2.3 (Distribution of a random variable)** Let $X$ be a random variable on $(\\Omega, \\mathcal{F}, P)$. The distribution of $X$ is the probability measure $P_X$ on $\\mathcal{B}(\\mathbb{R})$.\n",
    "\n",
    " So we can see that any random variable induces a probability measure on $\\mathcal{B}(\\mathbb{R})$. An important use of this concept through the cumulative distribution function (CDF) of $X$, defined as\n",
    " \n",
    " $$\n",
    " F_X(x) = P_X((-\\infty, x]) = P(X\\leq x), \\quad x \\in \\mathbb{R}\n",
    " $$ \n",
    "  \n",
    "**Example 2.4 (Cumulative effect of $F_X$)** Let $X$ be a random variable on $\\Omega = \\{1,2,3\\}$ with $P(X=1) = 0.1$, $P(X=2) = 0.3$ and $P(X=3) = 0.6$. Then, the CDF of $X$ is given by\n",
    " \n",
    " - $F_X(a) = 0$ for $a < 1$\n",
    " - $F_X(1) = 0.1$\n",
    " - $F_X(2) = 0.1 + 0.3 = 0.4$\n",
    " - $F_X(3) = 0.1 + 0.3 + 0.6 = 1$\n",
    " - $F_X(b) = 1$ for $b > 3$ \n",
    "  \n",
    "  $\\blacksquare$  \n",
    "   \n",
    "**Proposition 2.6 (Properties of the CDF)** Let $F_X$ be the CDF of a random variable $X$. Then,\n",
    "\n",
    "i. $F_X$ is non-decreasing.\n",
    "\n",
    "ii. $F_X$ is right-continuous, i.e., $\\lim_{x\\to x_0^+} F_X(x) = F_X(x_0)$ for all $x_0 \\in \\mathbb{R}$.\n",
    "\n",
    "iii. $\\lim_{x\\to-\\infty} F_X(x) = 0$ and $\\lim_{x\\to\\infty} F_X(x) = 1$.\n",
    "\n",
    "**Proof:** \n",
    "i. Let $x_1 < x_2$. Then, $(-\\infty, x_1] \\subset (-\\infty, x_2]$. Therefore, $F_X(x_1) = P(X\\leq x_1) \\leq P(X\\leq x_2) = F_X(x_2)$.\n",
    "\n",
    "ii. Let $x_n \\downarrow x_0$. Then, $(-\\infty, x_n] \\downarrow (-\\infty, x_0]$. Therefore, by the continuity of $P$, we have $F_X(x_n) \\downarrow F_X(x_0)$.\n",
    "\n",
    "iii. Let $x_n \\to -\\infty$. Then, $(-\\infty, x_n] \\to (-\\infty, -\\infty] = \\varnothing$. Therefore, $F_X(x_n) \\to 0$. Let $x_n \\to \\infty$. Then, $(-\\infty, x_n] \\to (-\\infty, \\infty) = \\mathbb{R}$. Therefore, $F_X(x_n) \\to 1$.\n",
    "\n",
    "\n",
    "$\\blacksquare$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
